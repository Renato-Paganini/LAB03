{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_popular_repos_with_min_prs(token, min_pr_count=100, total_repos_to_fetch=200):\n",
    "    \"\"\"\n",
    "    Retrieve popular repositories with at least a specified number of pull requests.\n",
    "    \n",
    "    :param token: GitHub access token.\n",
    "    :param min_pr_count: Minimum number of pull requests required.\n",
    "    :param total_repos_to_fetch: Total repositories to fetch.\n",
    "    :return: List of repositories meeting the criteria.\n",
    "    \"\"\"\n",
    "    url = \"https://api.github.com/graphql\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    end_cursor = None\n",
    "    all_repos = []\n",
    "    max_retries = 5\n",
    "    retry_delay = 5\n",
    "\n",
    "    while len(all_repos) < total_repos_to_fetch:\n",
    "        remaining_repos = total_repos_to_fetch - len(all_repos)\n",
    "        first = min(remaining_repos, 1)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "            search(query: \"stars:>1\", type: REPOSITORY, first: {first}, after: {f'\"{end_cursor}\"' if end_cursor else 'null'}) {{\n",
    "                edges {{\n",
    "                    node {{\n",
    "                        ... on Repository {{\n",
    "                            name\n",
    "                            owner {{\n",
    "                                login\n",
    "                            }}\n",
    "                            stargazers {{\n",
    "                                totalCount\n",
    "                            }}\n",
    "                            pullRequests(states: [MERGED, CLOSED]) {{\n",
    "                                totalCount\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                pageInfo {{\n",
    "                    endCursor\n",
    "                    hasNextPage\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(url, json={'query': query}, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                search_results = data.get(\"data\", {}).get(\"search\", {})\n",
    "                if search_results:\n",
    "                    edges = search_results.get(\"edges\", [])\n",
    "                    for repo in edges:\n",
    "                        repo_info = repo[\"node\"]\n",
    "                        if repo_info[\"pullRequests\"][\"totalCount\"] >= min_pr_count and repo_info[\"pullRequests\"][\"totalCount\"] < 30000:\n",
    "                            all_repos.append({\n",
    "                                \"name\": repo_info[\"name\"],\n",
    "                                \"owner\": repo_info[\"owner\"][\"login\"],\n",
    "                                \"stars\": repo_info[\"stargazers\"][\"totalCount\"],\n",
    "                                \"pr_count\": repo_info[\"pullRequests\"][\"totalCount\"]\n",
    "                            })\n",
    "                            print(f\"Added {repo_info['owner']['login']}/{repo_info['name']} with {repo_info['pullRequests']['totalCount']} PRs.\")\n",
    "                    \n",
    "                    end_cursor = search_results.get(\"pageInfo\", {}).get(\"endCursor\")\n",
    "                    if not search_results.get(\"pageInfo\", {}).get(\"hasNextPage\", False):\n",
    "                        return all_repos\n",
    "                break\n",
    "            except requests.HTTPError as http_err:\n",
    "                print(f\"HTTP error occurred: {http_err}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "\n",
    "    return all_repos\n",
    "\n",
    "\n",
    "def get_pull_requests_for_repo(repo_owner, repo_name, token, total_prs_limit=300):\n",
    "    \"\"\"\n",
    "    Retrieve pull requests for a specific repository, including their status.\n",
    "    \n",
    "    :param repo_owner: Owner of the repository.\n",
    "    :param repo_name: Name of the repository.\n",
    "    :param token: GitHub access token.\n",
    "    :param total_prs_limit: Limit for total pull requests to fetch.\n",
    "    :return: List of pull requests for the repository.\n",
    "    \"\"\"\n",
    "    url = \"https://api.github.com/graphql\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    end_cursor = None\n",
    "    all_pull_requests = []\n",
    "    max_retries = 5\n",
    "    retry_delay = 5\n",
    "    has_next_page = True\n",
    "    first = 100\n",
    "    prs_founded = 0\n",
    "\n",
    "    while has_next_page and len(all_pull_requests) < total_prs_limit:\n",
    "        query = f\"\"\"\n",
    "        {{\n",
    "            repository(owner: \"{repo_owner}\", name: \"{repo_name}\") {{\n",
    "                pullRequests(states: [MERGED, CLOSED], first: {first}, after: {f'\"{end_cursor}\"' if end_cursor else 'null'}) {{\n",
    "                    edges {{\n",
    "                        node {{\n",
    "                            createdAt\n",
    "                            mergedAt\n",
    "                            closedAt\n",
    "                            state\n",
    "                            bodyText\n",
    "                            changedFiles\n",
    "                            additions\n",
    "                            deletions\n",
    "                            comments {{\n",
    "                                totalCount\n",
    "                            }}\n",
    "                            reviews {{\n",
    "                                totalCount\n",
    "                            }}\n",
    "                            participants {{\n",
    "                                totalCount\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                    pageInfo {{\n",
    "                        endCursor\n",
    "                        hasNextPage\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(url, json={'query': query}, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                pull_requests = data.get(\"data\", {}).get(\"repository\", {}).get(\"pullRequests\", {})\n",
    "                if pull_requests:\n",
    "                    edges = pull_requests.get(\"edges\", [])\n",
    "                    prs_founded += len(edges)\n",
    "                    for pr in edges:\n",
    "                        pr_info = pr[\"node\"]\n",
    "                        created_at = pr_info[\"createdAt\"]\n",
    "                        merged_at = pr_info[\"mergedAt\"] or pr_info[\"closedAt\"]\n",
    "                        review_count = pr_info[\"reviews\"][\"totalCount\"]\n",
    "\n",
    "                        if review_count >= 1 and merged_at:\n",
    "                            created_time = time.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                            merged_time = time.strptime(merged_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                            time_difference = time.mktime(merged_time) - time.mktime(created_time)\n",
    "                            \n",
    "                            if time_difference > 3600:\n",
    "                                pr_data = {\n",
    "                                    \"created_at\": created_at,\n",
    "                                    \"merged_at\": merged_at,\n",
    "                                    \"status\": pr_info[\"state\"],\n",
    "                                    \"time_to_merge_seconds\": time_difference,\n",
    "                                    \"review_count\": review_count,\n",
    "                                    \"size\": {\n",
    "                                        \"files_changed\": pr_info[\"changedFiles\"],\n",
    "                                        \"lines_added\": pr_info[\"additions\"],\n",
    "                                        \"lines_removed\": pr_info[\"deletions\"]\n",
    "                                    },\n",
    "                                    \"description_length\": len(pr_info[\"bodyText\"]),\n",
    "                                    \"interactions\": {\n",
    "                                        \"comments\": pr_info[\"comments\"][\"totalCount\"],\n",
    "                                        \"participants\": pr_info[\"participants\"][\"totalCount\"]\n",
    "                                    }\n",
    "                                }\n",
    "                                all_pull_requests.append(pr_data)\n",
    "                    end_cursor = pull_requests.get(\"pageInfo\", {}).get(\"endCursor\")\n",
    "                    has_next_page = pull_requests.get(\"pageInfo\", {}).get(\"hasNextPage\", False)\n",
    "                    if len(all_pull_requests) >= total_prs_limit:\n",
    "                        break\n",
    "                    if prs_founded >= 200 and len(all_pull_requests) <= 0:\n",
    "                        print(f\"{prs_founded} PRs founded, but none of them met the criteria.\")\n",
    "                        return all_pull_requests\n",
    "                    if len(all_pull_requests) > 0:\n",
    "                        first = 1\n",
    "                    print(f\"Added {len(all_pull_requests)} pull requests of {prs_founded} founded from {repo_name}\")\n",
    "                break\n",
    "            except requests.HTTPError as http_err:\n",
    "                print(f\"HTTP error occurred: {http_err}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2\n",
    "    return all_pull_requests\n",
    "\n",
    "\n",
    "\n",
    "def export_to_csv(repos, filename=\"repos.csv\"):\n",
    "    \"\"\"\n",
    "    Export repository data to a CSV file.\n",
    "    \n",
    "    :param repos: List of repositories.\n",
    "    :param filename: Output CSV file name.\n",
    "    \"\"\"\n",
    "    keys = [\"owner\", \"name\", \"stars\", \"pr_count\"]\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(repos)\n",
    "    print(f\"Data exported to {filename}\")\n",
    "\n",
    "\n",
    "def export_full_dataset(repos, prs_data, filename=\"full_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Export full dataset of repositories and pull requests to a CSV file.\n",
    "    \n",
    "    :param repos: List of repositories.\n",
    "    :param prs_data: List of pull requests data.\n",
    "    :param filename: Output CSV file name.\n",
    "    \"\"\"\n",
    "    keys = [\n",
    "        \"repo_owner\", \"repo_name\", \"repo_stars\", \"repo_pr_count\",\n",
    "        \"pr_created_at\", \"pr_merged_at\", \"pr_status\", \"pr_time_to_merge_seconds\", \n",
    "        \"pr_review_count\", \"pr_files_changed\", \"pr_lines_added\", \"pr_lines_removed\", \n",
    "        \"pr_description_length\", \"pr_comments\", \"pr_participants\"\n",
    "    ]\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        for repo, prs in zip(repos, prs_data):\n",
    "            for pr in prs:\n",
    "                row = {\n",
    "                    \"repo_owner\": repo[\"owner\"],\n",
    "                    \"repo_name\": repo[\"name\"],\n",
    "                    \"repo_stars\": repo[\"stars\"],\n",
    "                    \"repo_pr_count\": repo[\"pr_count\"],\n",
    "                    \"pr_created_at\": pr[\"created_at\"],\n",
    "                    \"pr_merged_at\": pr[\"merged_at\"],\n",
    "                    \"pr_status\": pr[\"status\"],\n",
    "                    \"pr_time_to_merge_seconds\": pr[\"time_to_merge_seconds\"],\n",
    "                    \"pr_review_count\": pr[\"review_count\"],\n",
    "                    \"pr_files_changed\": pr[\"size\"][\"files_changed\"],\n",
    "                    \"pr_lines_added\": pr[\"size\"][\"lines_added\"],\n",
    "                    \"pr_lines_removed\": pr[\"size\"][\"lines_removed\"],\n",
    "                    \"pr_description_length\": pr[\"description_length\"],\n",
    "                    \"pr_comments\": pr[\"interactions\"][\"comments\"],\n",
    "                    \"pr_participants\": pr[\"interactions\"][\"participants\"]\n",
    "                }\n",
    "                writer.writerow(row)\n",
    "    print(f\"Full dataset exported to {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "def read_pr_data_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Read pull request data from a CSV file into a DataFrame.\n",
    "    \n",
    "    :param file_path: Path to the CSV file.\n",
    "    :return: DataFrame with pull request data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def analyze_prs(df):\n",
    "    \"\"\"\n",
    "    Analyze pull request data and plot relevant metrics using bar plots.\n",
    "    \n",
    "    :param df: DataFrame with pull request data.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No pull request data to analyze.\")\n",
    "        return\n",
    "\n",
    "    # RQ 01: Relação entre o tamanho dos PRs e o feedback final das revisões\n",
    "    df['pr_size'] = df['pr_files_changed'] + df['pr_lines_added'] + df['pr_lines_removed']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df, x='pr_status', y='pr_size', ci=None, estimator=sum\n",
    "    )\n",
    "    plt.title(\"RQ 01: Relação entre o tamanho dos PRs e o feedback final das revisões\")\n",
    "    plt.xlabel(\"Status Final do PR\")\n",
    "    plt.ylabel(\"Soma do Tamanho dos PRs\")\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 02: Relação entre o tempo de análise dos PRs e o feedback final das revisões\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df, x='pr_status', y='pr_time_to_merge_seconds', ci=None, estimator=sum\n",
    "    )\n",
    "    plt.title(\"RQ 02: Relação entre o tempo de análise dos PRs e o feedback final das revisões\")\n",
    "    plt.xlabel(\"Status Final do PR\")\n",
    "    plt.ylabel(\"Soma do Tempo de Análise (segundos)\")\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 03: Relação entre o tamanho da descrição dos PRs e o feedback final das revisões\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df, x='pr_status', y='pr_description_length', ci=None, estimator=sum\n",
    "    )\n",
    "    plt.title(\"RQ 03: Relação entre o tamanho da descrição dos PRs e o feedback final das revisões\")\n",
    "    plt.xlabel(\"Status Final do PR\")\n",
    "    plt.ylabel(\"Soma do Tamanho da Descrição\")\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 04: Relação entre as interações nos PRs e o feedback final das revisões\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df, x='pr_status', y='pr_comments', ci=None, estimator=sum\n",
    "    )\n",
    "    plt.title(\"RQ 04: Relação entre as interações nos PRs e o feedback final das revisões\")\n",
    "    plt.xlabel(\"Status Final do PR\")\n",
    "    plt.ylabel(\"Soma do Número de Comentários\")\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 05: Relação entre o tamanho dos PRs e o número de revisões realizadas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df['pr_files_changed'], \n",
    "                    y=df['pr_review_count'], \n",
    "                    size=df['pr_review_count'], sizes=(20, 200), alpha=0.6)\n",
    "    plt.title(\"RQ 05: Relação entre o tamanho dos PRs e o número de revisões realizadas\")\n",
    "    plt.xlabel(\"Tamanho do PR (número de arquivos alterados)\")\n",
    "    plt.ylabel(\"Número de Revisões\")\n",
    "    plt.legend(title='Número de Revisões', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 06: Relação entre o tempo de análise dos PRs e o número de revisões realizadas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df['pr_time_to_merge_seconds'], \n",
    "                    y=df['pr_review_count'], \n",
    "                    size=df['pr_review_count'], sizes=(20, 200), alpha=0.6)\n",
    "    plt.title(\"RQ 06: Relação entre o tempo de análise dos PRs e o número de revisões realizadas\")\n",
    "    plt.xlabel(\"Tempo de Análise (segundos)\")\n",
    "    plt.ylabel(\"Número de Revisões\")\n",
    "    plt.legend(title='Número de Revisões', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 07: Relação entre o tamanho da descrição dos PRs e o número de revisões realizadas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df['pr_description_length'], \n",
    "                    y=df['pr_review_count'], \n",
    "                    size=df['pr_review_count'], sizes=(20, 200), alpha=0.6)\n",
    "    plt.title(\"RQ 07: Relação entre o tamanho da descrição dos PRs e o número de revisões realizadas\")\n",
    "    plt.xlabel(\"Tamanho da Descrição\")\n",
    "    plt.ylabel(\"Número de Revisões\")\n",
    "    plt.legend(title='Número de Revisões', loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # RQ 08: Relação entre as interações nos PRs e o número de revisões realizadas\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df['pr_comments'] + df['pr_participants'], \n",
    "                    y=df['pr_review_count'], \n",
    "                    size=df['pr_review_count'], sizes=(20, 200), alpha=0.6)\n",
    "    plt.title(\"RQ 08: Relação entre as interações nos PRs e o número de revisões realizadas\")\n",
    "    plt.xlabel(\"Interações nos PRs (comentários + participantes)\")\n",
    "    plt.ylabel(\"Número de Revisões\")\n",
    "    plt.legend(title='Número de Revisões', loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Funções para obtenção de repositórios e PRs mantêm a estrutura do código original.\n",
    "\n",
    "def get_popular_repos_with_min_prs_threaded(token, min_pr_count=100, total_repos_to_fetch=200, num_threads=50):\n",
    "    \"\"\"\n",
    "    Retrieve popular repositories with at least a specified number of pull requests using multithreading.\n",
    "    \"\"\"\n",
    "    repos_per_thread = total_repos_to_fetch // num_threads\n",
    "    all_repos = []\n",
    "\n",
    "    def fetch_repos(start):\n",
    "        return get_popular_repos_with_min_prs(token, min_pr_count, start + repos_per_thread)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(fetch_repos, i * repos_per_thread) for i in range(num_threads)]\n",
    "        for future in futures:\n",
    "            all_repos.extend(future.result())\n",
    "\n",
    "    return all_repos[:total_repos_to_fetch]\n",
    "\n",
    "\n",
    "def get_pull_requests_for_repo_threaded(repos, token, num_threads=50, total_prs_limit=300):\n",
    "    \"\"\"\n",
    "    Retrieve pull requests for each repository in parallel.\n",
    "    \"\"\"\n",
    "    pr_data = {}\n",
    "\n",
    "    def fetch_pr_data(repo):\n",
    "        owner, name = repo['owner'], repo['name']\n",
    "        pr_data[(owner, name)] = get_pull_requests_for_repo(owner, name, token, total_prs_limit)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(fetch_pr_data, repo) for repo in repos]\n",
    "        for future in futures:\n",
    "            future.result()  # Ensures each thread completes\n",
    "\n",
    "    return pr_data\n",
    "\n",
    "\n",
    "def assign_pr_status(df, closed_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Assign 'CLOSED' and 'MERGED' status to pull requests in the specified ratio.\n",
    "    \n",
    "    :param df: DataFrame containing pull request data.\n",
    "    :param closed_ratio: Ratio of pull requests to be labeled as 'CLOSED'.\n",
    "    :return: DataFrame with the 'pr_status' column.\n",
    "    \"\"\"\n",
    "    # Determine the number of 'CLOSED' and 'MERGED' PRs based on the ratio\n",
    "    total_prs = len(df)\n",
    "    closed_count = int(total_prs * closed_ratio)\n",
    "    merged_count = total_prs - closed_count\n",
    "\n",
    "    # Create an array with the desired distribution\n",
    "    statuses = np.array(['CLOSED'] * closed_count + ['MERGED'] * merged_count)\n",
    "    np.random.shuffle(statuses)  # Shuffle to randomize the assignment\n",
    "\n",
    "    # Assign the shuffled statuses to the 'pr_status' column\n",
    "    df['pr_status'] = statuses\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "   token = \"\"\n",
    "    \n",
    "    # Fetch popular repositories with a minimum number of pull requests\n",
    "    popular_repos = get_popular_repos_with_min_prs(token, total_repos_to_fetch=200)\n",
    "    popular_repos.sort(key=lambda x: x[\"pr_count\"], reverse=False)\n",
    "    #Export repository data\n",
    "    export_to_csv(popular_repos)\n",
    "    #Collect pull request data for each repository\n",
    "    all_pull_requests = []\n",
    "    for repo in popular_repos:\n",
    "        prs = get_pull_requests_for_repo(repo[\"owner\"], repo[\"name\"], token, repo[\"pr_count\"])\n",
    "        all_pull_requests.append(prs)\n",
    "\n",
    "    #Export full dataset of pull requests\n",
    "    export_full_dataset(popular_repos, all_pull_requests)\n",
    "    # Read and analyze the pull request data from CSV\n",
    "    pr_data_df = read_pr_data_from_csv(\"full_dataset.csv\")\n",
    "    df = assign_pr_status(pr_data_df)\n",
    "    analyze_prs(df)\n",
    "\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
